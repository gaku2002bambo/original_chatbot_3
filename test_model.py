import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

def test_finetuned_model():
    print("ğŸ§ª ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ...")
    
    # ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®š
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("ğŸ Apple Silicon GPU (MPS) ã‚’ä½¿ç”¨ã—ã¾ã™")
    else:
        device = torch.device("cpu")
        print("ğŸ’» CPUã‚’ä½¿ç”¨ã—ã¾ã™")
    
    # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    base_model_name = "cyberagent/open-calm-small"  # iOSå‘ã‘è»½é‡ãƒ¢ãƒ‡ãƒ«
    model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float32,  # MPSã¯float32
        trust_remote_code=True
    ).to(device)
    
    # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰
    model = PeftModel.from_pretrained(model, "./finetuned_model")
    tokenizer = AutoTokenizer.from_pretrained("./finetuned_model", trust_remote_code=True)
    
    # ãƒ†ã‚¹ãƒˆè³ªå•
    test_questions = [
        "å¥½ããªé£Ÿã¹ç‰©ã¯ä½•ï¼Ÿ",
        "é€±æœ«ã¯ä½•ã—ã¦éã”ã—ãŸã„ï¼Ÿ",
        "æœ€è¿‘å¬‰ã—ã‹ã£ãŸã“ã¨ã¯ï¼Ÿ",
        "ç§ã®ã“ã¨å¥½ãï¼Ÿ"
    ]
    
    print("\nğŸ“ ãƒ†ã‚¹ãƒˆçµæœ:")
    print("-" * 50)
    
    for question in test_questions:
        prompt = f"è³ªå•: {question}\nå›ç­”: "
        inputs = tokenizer(prompt, return_tensors="pt")
        
        # inputsã‚’ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = response.split("å›ç­”: ")[-1]
        
        print(f"Q: {question}")
        print(f"A: {answer}")
        print("-" * 50)

if __name__ == "__main__":
    test_finetuned_model()