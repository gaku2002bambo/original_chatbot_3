import torch
import torch.onnx
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import numpy as np

def export_to_onnx():
    print("ğŸ”„ ONNXå½¢å¼ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–‹å§‹...")
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device("cpu")  # ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã¯CPUã§å®Ÿè¡Œ
    
    # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    print("ğŸ“¦ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    base_model_name = "cyberagent/open-calm-small"
    model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float32,
        trust_remote_code=True
    ).to(device)
    
    # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒãƒ¼ã‚¸
    model = PeftModel.from_pretrained(model, "./finetuned_model")
    model = model.merge_and_unload()  # LoRAã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ¼ã‚¸
    model.eval()
    
    tokenizer = AutoTokenizer.from_pretrained("./finetuned_model", trust_remote_code=True)
    
    # å›ºå®šé•·ã®å…¥åŠ›ã‚’æº–å‚™
    print("ğŸ“ ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›ã‚’æº–å‚™ä¸­...")
    dummy_input = torch.randint(0, tokenizer.vocab_size, (1, 20)).to(device)
    
    # ONNXã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªforward passã®ã¿ï¼‰
    print("ğŸ”„ ONNXã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­...")
    
    # ã‚ˆã‚Šå˜ç´”ãªãƒ©ãƒƒãƒ‘ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    class SimpleModel(torch.nn.Module):
        def __init__(self, model):
            super().__init__()
            self.model = model
            
        def forward(self, input_ids):
            # å˜ç´”ã«logitsã®ã¿ã‚’è¿”ã™
            outputs = self.model(input_ids, use_cache=False)
            return outputs.logits
    
    simple_model = SimpleModel(model)
    simple_model.eval()
    
    # ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    torch.onnx.export(
        simple_model,
        dummy_input,
        "model.onnx",
        export_params=True,
        opset_version=11,
        do_constant_folding=True,
        input_names=['input_ids'],
        output_names=['logits'],
        dynamic_axes={
            'input_ids': {0: 'batch_size', 1: 'sequence'},
            'logits': {0: 'batch_size', 1: 'sequence'}
        }
    )
    
    print("âœ… ONNXãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: model.onnx")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èªå½™ã‚’ä¿å­˜
    import json
    vocab = tokenizer.get_vocab()
    with open("tokenizer_vocab.json", "w", encoding="utf-8") as f:
        json.dump(vocab, f, ensure_ascii=False, indent=2)
    
    # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚‚ä¿å­˜
    special_tokens = {
        "pad_token": tokenizer.pad_token,
        "eos_token": tokenizer.eos_token,
        "bos_token": tokenizer.bos_token if hasattr(tokenizer, 'bos_token') else None,
        "unk_token": tokenizer.unk_token if hasattr(tokenizer, 'unk_token') else None,
    }
    with open("special_tokens.json", "w", encoding="utf-8") as f:
        json.dump(special_tokens, f, ensure_ascii=False, indent=2)
    
    print("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
    print("\nğŸ“± æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. onnx2tf ã§TensorFlow Liteã«å¤‰æ›")
    print("2. Core MLToolsã§Core MLã«å¤‰æ›")
    print("3. iOSã‚¢ãƒ—ãƒªã«çµ„ã¿è¾¼ã¿")

if __name__ == "__main__":
    export_to_onnx()