import torch
import coremltools as ct
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import numpy as np

def convert_to_coreml():
    print("ğŸ Core MLå¤‰æ›ã‚’é–‹å§‹ã—ã¾ã™...")
    
    # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    base_model_name = "cyberagent/open-calm-small"
    model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float32,
        trust_remote_code=True
    )
    
    # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰
    model = PeftModel.from_pretrained(model, "./finetuned_model")
    model = model.merge_and_unload()  # LoRAã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ¼ã‚¸
    model.eval()
    
    tokenizer = AutoTokenizer.from_pretrained("./finetuned_model", trust_remote_code=True)
    
    print("ğŸ“¦ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›ä¸­...")
    
    # ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›ã‚’ä½œæˆ
    sample_text = "è³ªå•: ã“ã‚“ã«ã¡ã¯\nå›ç­”: "
    inputs = tokenizer(sample_text, return_tensors="pt")
    input_ids = inputs["input_ids"]
    
    # TorchScriptã«å¤‰æ›
    traced_model = torch.jit.trace(model, input_ids)
    
    print("ğŸ”„ Core MLã«å¤‰æ›ä¸­...")
    
    # Core MLã«å¤‰æ›
    mlmodel = ct.convert(
        traced_model,
        inputs=[ct.TensorType(shape=input_ids.shape, dtype=np.int32)],
        minimum_deployment_target=ct.target.iOS16,
        convert_to="mlprogram"
    )
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    mlmodel.save("ChatModel.mlpackage")
    print("âœ… Core MLãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: ChatModel.mlpackage")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èªå½™ã‚’ä¿å­˜
    import json
    vocab = tokenizer.get_vocab()
    with open("tokenizer_vocab.json", "w", encoding="utf-8") as f:
        json.dump(vocab, f, ensure_ascii=False, indent=2)
    print("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èªå½™ã‚’ä¿å­˜ã—ã¾ã—ãŸ: tokenizer_vocab.json")

if __name__ == "__main__":
    convert_to_coreml()