import json
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
from peft import LoraConfig, get_peft_model, TaskType
import os

def load_conversation_data(file_path):
    conversations = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            text = f"è³ªå•: {data['instruction']}\nå›ç­”: {data['output']}"
            conversations.append({"text": text})
    return conversations

def main():
    print("ğŸš€ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™...")
    
    # ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®šï¼ˆM4 Macç”¨ï¼‰
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("ğŸ Apple Silicon GPU (MPS) ã‚’ä½¿ç”¨ã—ã¾ã™")
    else:
        device = torch.device("cpu")
        print("ğŸ’» CPUã‚’ä½¿ç”¨ã—ã¾ã™")
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¨­å®š
    # iOSå‘ã‘ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    model_name = "cyberagent/open-calm-small"  # 160Mï¼ˆiOSæœ€é©ãƒ»æ¨å¥¨ï¼‰
    # model_name = "rinna/japanese-gpt2-small"  # 110Mï¼ˆæœ€è»½é‡ï¼‰
    # model_name = "rinna/japanese-gpt2-medium"  # 336Mï¼ˆå“è³ªé‡è¦–ï¼‰
    # model_name = "microsoft/Phi-3-mini-4k-instruct"  # 3.8Bï¼ˆiOSã«ã¯å¤§ãã™ãã‚‹ï¼‰
    
    print(f"ğŸ“¦ ãƒ¢ãƒ‡ãƒ« {model_name} ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float32,  # MPSã¯float32ã‚’ä½¿ç”¨
        trust_remote_code=True
    ).to(device)
    
    # LoRAè¨­å®š
    print("âš™ï¸ LoRAè¨­å®šã‚’é©ç”¨ä¸­...")
    
    # ãƒ¢ãƒ‡ãƒ«ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã‚’ç¢ºèª
    print("ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«:")
    for name, module in model.named_modules():
        if len(list(module.children())) == 0:  # ãƒªãƒ¼ãƒ•ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿
            print(f"  - {name}: {module.__class__.__name__}")
            if "query" in name or "key" in name or "value" in name or "dense" in name:
                break
    
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=8,
        lora_alpha=16,
        lora_dropout=0.1,
        # OpenCALMç”¨ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆè‡ªå‹•æ¤œå‡ºã•ã‚Œã‚‹ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨ï¼‰
        target_modules=None  # Noneã«ã™ã‚‹ã¨è‡ªå‹•çš„ã«é©åˆ‡ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒé¸æŠã•ã‚Œã‚‹
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
    print("ğŸ“š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ä¸­...")
    conversations = load_conversation_data("conversation_data.jsonl")
    
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=256
        )
    
    dataset = Dataset.from_list(conversations)
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã®è¨­å®š
    training_args = TrainingArguments(
        output_dir="./checkpoints",
        num_train_epochs=10,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=100,
        logging_steps=10,
        save_steps=500,
        save_total_limit=2,
        use_mps_device=torch.backends.mps.is_available(),  # MPSä½¿ç”¨
        fp16=False,  # MPSã§ã¯fp16ã¯ä½¿ç”¨ã—ãªã„
        push_to_hub=False,
    )
    
    # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã®è¨­å®š
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®è¨­å®š
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
    )
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹
    print("ğŸ‹ï¸ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...")
    trainer.train()
    
    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    print("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­...")
    model.save_pretrained("./finetuned_model")
    tokenizer.save_pretrained("./finetuned_model")
    
    print("âœ¨ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼")

if __name__ == "__main__":
    main()